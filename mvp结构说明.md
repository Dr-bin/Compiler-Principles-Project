# ç¼–è¯‘å™¨ç”Ÿæˆå™¨ - å®Œæ•´é¡¹ç›®ç»“æ„

ä¸‹é¢æ˜¯ä¸€ä¸ªç¬¦åˆè¦æ±‚çš„å®Œæ•´MVPé¡¹ç›®ç»“æ„ï¼Œæ‰€æœ‰è¾“å…¥è¾“å‡ºæ–¹å¼éƒ½ä¸æœ€ç»ˆé¡¹ç›®ä¸€è‡´ã€‚

## é¡¹ç›®ç»“æ„

```
compiler_project/
â”œâ”€â”€ src/                          # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ compiler_generator/       # ç¼–è¯‘å™¨ç”Ÿæˆå™¨æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lexer_generator.py    # è¯æ³•åˆ†æå™¨ç”Ÿæˆå™¨ (åŒå­¦A)
â”‚   â”‚   â”œâ”€â”€ parser_generator.py   # è¯­æ³•åˆ†æå™¨ç”Ÿæˆå™¨ (åŒå­¦B)
â”‚   â”‚   â””â”€â”€ code_generator.py     # ä»£ç ç”Ÿæˆå™¨ (åŒå­¦C)
â”‚   â”œâ”€â”€ frontend/                 # å‰ç«¯æ¥å£
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rule_parser.py        # è§„åˆ™æ–‡ä»¶è§£æå™¨
â”‚   â”‚   â””â”€â”€ cli.py               # å‘½ä»¤è¡Œæ¥å£
â”‚   â””â”€â”€ utils/                   # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ error_handler.py     # é”™è¯¯å¤„ç†
â”‚       â””â”€â”€ logger.py            # æ—¥å¿—ç³»ç»Ÿ
â”œâ”€â”€ examples/                    # ç¤ºä¾‹è¯­è¨€å®šä¹‰
â”‚   â”œâ”€â”€ simple_expr/            # ç®€å•è¡¨è¾¾å¼è¯­è¨€
â”‚   â”‚   â”œâ”€â”€ lexer_rules.txt     # è¯æ³•è§„åˆ™
â”‚   â”‚   â””â”€â”€ grammar_rules.txt   # è¯­æ³•è§„åˆ™
â”‚   â””â”€â”€ pl0_subset/             # PL/0å­é›†è¯­è¨€
â”‚       â”œâ”€â”€ lexer_rules.txt
â”‚       â””â”€â”€ grammar_rules.txt
â”œâ”€â”€ generated/                   # ç”Ÿæˆçš„ç¼–è¯‘å™¨è¾“å‡ºç›®å½•
â”œâ”€â”€ tests/                      # æµ‹è¯•ç”¨ä¾‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_lexer.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ main.py                     # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## 1. è§„åˆ™æ–‡ä»¶æ ¼å¼

### è¯æ³•è§„åˆ™æ–‡ä»¶ (examples/simple_expr/lexer_rules.txt)

**å®é™…æ ¼å¼è¯´æ˜**ï¼šæ¯è¡Œä¸€ä¸ªè¯æ³•è§„åˆ™ï¼Œæ ¼å¼ä¸º `TOKEN_TYPE = regex_pattern`ï¼Œæ³¨é‡Šç”¨ `#` å¼€å¤´ã€‚

```
# ç®€å•è¡¨è¾¾å¼è¯­è¨€çš„è¯æ³•è§„åˆ™
# æ ¼å¼: TOKEN_TYPE = regex_pattern

# å…³é”®å­—ï¼ˆå¿…é¡»æ”¾åœ¨IDä¹‹å‰ï¼‰
PRINT = print

# æ ‡è¯†ç¬¦
ID = [a-zA-Z_][a-zA-Z0-9_]*

# æ•°å­—ï¼ˆæ•´æ•°å’Œæµ®ç‚¹æ•°ï¼‰
NUM = [0-9]+(?:\.[0-9]+)?

# ç®—æœ¯è¿ç®—ç¬¦
PLUS = \+
MINUS = -
MUL = \*
DIV = /

# èµ‹å€¼å·
ASSIGN = =

# æ‹¬å·
LPAREN = \(
RPAREN = \)

# åˆ†å·
SEMI = ;
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- å…³é”®å­—å¿…é¡»æ”¾åœ¨æ ‡è¯†ç¬¦ä¹‹å‰ï¼ˆä¼˜å…ˆåŒ¹é…ï¼‰
- æ”¯æŒæ ‡å‡†æ­£åˆ™è¡¨è¾¾å¼è¯­æ³•
- ç©ºç™½ç¬¦è‡ªåŠ¨è·³è¿‡
- æ³¨é‡Šè¡Œä»¥ `#` å¼€å¤´

### è¯­æ³•è§„åˆ™æ–‡ä»¶ (examples/simple_expr/grammar_rules.txt)

**å®é™…æ ¼å¼è¯´æ˜**ï¼šä½¿ç”¨ç®€åŒ–çš„ BNF æ ¼å¼ï¼Œäº§ç”Ÿå¼å³ä¾§ç¬¦å·ç”¨ç©ºæ ¼åˆ†éš”ï¼Œç»ˆç»“ç¬¦ç”¨å•å¼•å·æ‹¬èµ·æ¥ã€‚

```
# ç®€å•è¡¨è¾¾å¼è¯­è¨€çš„è¯­æ³•è§„åˆ™
# æ ¼å¼: NonTerminal -> production1 | production2

# ç¨‹åºï¼šç”±è¯­å¥ç»„æˆ
Program -> StmtList

# è¯­å¥åˆ—è¡¨ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¯­å¥
# æ³¨æ„ï¼šé€’å½’äº§ç”Ÿå¼åº”è¯¥æ”¾åœ¨å‰é¢ä»¥ä¾¿æ­£ç¡®å›æº¯
StmtList -> Stmt StmtList | Stmt

# è¯­å¥ï¼šèµ‹å€¼æˆ–æ‰“å°
Stmt -> 'ID' 'ASSIGN' Expr 'SEMI' | 'PRINT' 'LPAREN' Expr 'RPAREN' 'SEMI'

# è¡¨è¾¾å¼ï¼šæ”¯æŒåŠ å‡ä¹˜é™¤ï¼ˆä½¿ç”¨è¾…åŠ©éç»ˆç»“ç¬¦ç®¡ç†è¿ç®—ç¬¦ï¼‰
Expr -> Term AddOp | Term

AddOp -> 'PLUS' Term AddOp | 'MINUS' Term AddOp | 'PLUS' Term | 'MINUS' Term

Term -> Factor MulOp | Factor

MulOp -> 'MUL' Factor MulOp | 'DIV' Factor MulOp | 'MUL' Factor | 'DIV' Factor

# å› å­ï¼šæ•°å­—ã€æ ‡è¯†ç¬¦æˆ–æ‹¬å·è¡¨è¾¾å¼
Factor -> 'NUM' | 'ID' | 'LPAREN' Expr 'RPAREN'
```

**å…³é”®æ”¹è¿›**ï¼š
- ä½¿ç”¨äº† `AddOp` å’Œ `MulOp` ä»£æ›¿å³é€’å½’ï¼Œé¿å…äº†æ— é™é€’å½’é—®é¢˜
- äº§ç”Ÿå¼é¡ºåºä¼˜åŒ–ï¼šé€’å½’äº§ç”Ÿå¼æ”¾åœ¨å‰é¢ï¼ŒåŸºç¡€äº§ç”Ÿå¼æ”¾åœ¨åé¢
- è¿™æ ·é€’å½’ä¸‹é™è§£æå™¨èƒ½æ­£ç¡®è¿›è¡Œå›æº¯å’Œé€‰æ‹©

## 2. æ ¸å¿ƒä»£ç å®ç°æ¦‚è§ˆ

### 2.1 å®ç°äº®ç‚¹

#### è¯æ³•åˆ†æå™¨ (lexer_generator.py - åŒå­¦A)
- **åŠŸèƒ½**ï¼šå°†æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™è‡ªåŠ¨ç¼–è¯‘ä¸ºè¯æ³•æ‰«æå™¨
- **è¾“å‡º**ï¼šToken åºåˆ—ï¼ŒåŒ…æ‹¬ç±»å‹ã€å€¼ã€è¡Œåˆ—ä½ç½®
- **ç‰¹æ€§**ï¼šæ”¯æŒä½ç½®è¿½è¸ªã€é”™è¯¯å¤„ç†ã€ç©ºç™½ç¬¦è¿‡æ»¤

#### è¯­æ³•åˆ†æå™¨ (parser_generator.py - åŒå­¦B)  
- **åŠŸèƒ½**ï¼šå°† BNF æ–‡æ³•è§„èŒƒè½¬æ¢ä¸ºé€’å½’ä¸‹é™è§£æå™¨
- **è¾“å‡º**ï¼šæŠ½è±¡è¯­æ³•æ ‘ (AST)
- **ç‰¹æ€§**ï¼šæ”¯æŒå›æº¯ã€å¤šäº§ç”Ÿå¼é€‰æ‹©ã€é”™è¯¯æ¢å¤

#### ä»£ç ç”Ÿæˆå™¨ (code_generator.py - åŒå­¦C)
- **åŠŸèƒ½**ï¼šä» AST ç”Ÿæˆä¸‰åœ°å€ä¸­é—´ä»£ç 
- **è¾“å‡º**ï¼šä¸‰åœ°å€ç ï¼ˆTACï¼‰æŒ‡ä»¤åºåˆ—
- **ç‰¹æ€§**ï¼šæ”¯æŒè¡¨è¾¾å¼è®¡ç®—ã€å˜é‡èµ‹å€¼ã€å‡½æ•°è°ƒç”¨

### 2.2 å®ç°ç»†èŠ‚

#### Token ç±»ï¼ˆè¯æ³•åˆ†æå™¨ï¼‰
```python
# Token ç”¨äºè¡¨ç¤ºä¸€ä¸ªè¯æ³•å•å…ƒ
Token(type, value, line, column)
# ä¾‹å¦‚ï¼šToken(ID, 'x', 1, 1)
```

#### ASTNode ç±»ï¼ˆè¯­æ³•åˆ†æå™¨ï¼‰
```python
# ASTNode ç”¨äºè¡¨ç¤ºè¯­æ³•æ ‘ä¸­çš„èŠ‚ç‚¹
ASTNode(name, children, token)
# ä¾‹å¦‚ï¼šASTNode('Expr', [Term_node, AddOp_node], None)
```

#### ä¸‰åœ°å€ç æ ¼å¼ï¼ˆä»£ç ç”Ÿæˆå™¨ï¼‰
```
# èµ‹å€¼è¯­å¥
x = 10

# äºŒå…ƒè¿ç®—
t1 = x + y

# å‡½æ•°è°ƒç”¨
print(t1)
```

### 2.3 å®Œæ•´çš„ç¼–è¯‘æµç¨‹

```
æºä»£ç  â†’ è¯æ³•åˆ†æ â†’ Tokenåºåˆ— â†’ è¯­æ³•åˆ†æ â†’ AST â†’ ä»£ç ç”Ÿæˆ â†’ ä¸‰åœ°å€ç 
```

**ç¤ºä¾‹**ï¼šç¼–è¯‘ `x = 10 + 20 ;`

```
1. è¯æ³•åˆ†æï¼š
   Token(ID, 'x'), Token(ASSIGN, '='), Token(NUM, '10'), 
   Token(PLUS, '+'), Token(NUM, '20'), Token(SEMI, ';'), Token(EOF, '')

2. è¯­æ³•åˆ†æï¼š
   Program
   â””â”€ StmtList
      â””â”€ Stmt
         â”œâ”€ 'ID' ('x')
         â”œâ”€ 'ASSIGN' ('=')
         â”œâ”€ Expr
         â”‚  â”œâ”€ Term
         â”‚  â”‚  â””â”€ Factor
         â”‚  â”‚     â””â”€ 'NUM' ('10')
         â”‚  â””â”€ AddOp
         â”‚     â”œâ”€ 'PLUS' ('+')
         â”‚     â””â”€ Term
         â”‚        â””â”€ Factor
         â”‚           â””â”€ 'NUM' ('20')
         â””â”€ 'SEMI' (';')

3. ä»£ç ç”Ÿæˆï¼š
   t1 = 10 + 20
   x = t1
```

```python
#!/usr/bin/env python3
"""
ç¼–è¯‘å™¨ç”Ÿæˆå™¨ä¸»ç¨‹åºå…¥å£

ä½¿ç”¨æ–¹æ³•:
1. ç”Ÿæˆç¼–è¯‘å™¨: python main.py generate --lexer examples/simple_expr/lexer_rules.txt --grammar examples/simple_expr/grammar_rules.txt --output generated/simple_expr_compiler.py
2. ä½¿ç”¨ç”Ÿæˆçš„ç¼–è¯‘å™¨: python generated/simple_expr_compiler.py compile --input examples/test_source.txt --output output.tac
"""

import argparse
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from frontend.cli import CompilerGeneratorCLI

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¼–è¯‘å™¨ç”Ÿæˆå™¨")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # generateå‘½ä»¤: ç”Ÿæˆç¼–è¯‘å™¨
    generate_parser = subparsers.add_parser("generate", help="ç”Ÿæˆç¼–è¯‘å™¨")
    generate_parser.add_argument("--lexer", "-l", required=True, help="è¯æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„")
    generate_parser.add_argument("--grammar", "-g", required=True, help="è¯­æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„")
    generate_parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºç¼–è¯‘å™¨æ–‡ä»¶è·¯å¾„")
    
    # å¦‚æœç›´æ¥è¿è¡Œç”Ÿæˆçš„ç¼–è¯‘å™¨
    if len(sys.argv) > 1 and sys.argv[1] == "compile":
        # è¿™é‡Œå‡è®¾æˆ‘ä»¬æ˜¯è¢«ç”Ÿæˆçš„ç¼–è¯‘å™¨è°ƒç”¨çš„
        compile_parser = argparse.ArgumentParser(description="ç”Ÿæˆçš„ç¼–è¯‘å™¨")
        compile_parser.add_argument("--input", "-i", required=True, help="æºä»£ç æ–‡ä»¶è·¯å¾„")
        compile_parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
        args = compile_parser.parse_args()
        
        # è¿™é‡Œåº”è¯¥æ˜¯ç”Ÿæˆçš„ç¼–è¯‘å™¨çš„ç¼–è¯‘é€»è¾‘
        print(f"ç¼–è¯‘ {args.input} -> {args.output}")
        # å®é™…å®ç°ä¼šåœ¨ç”Ÿæˆçš„ç¼–è¯‘å™¨ä¸­
    else:
        # è¿è¡Œç¼–è¯‘å™¨ç”Ÿæˆå™¨
        args = parser.parse_args()
        
        if args.command == "generate":
            cli = CompilerGeneratorCLI()
            cli.generate_compiler(args.lexer, args.grammar, args.output)
        else:
            parser.print_help()

if __name__ == "__main__":
    main()
```

### 2.2 å‘½ä»¤è¡Œæ¥å£ (src/frontend/cli.py)

```python
"""
å‘½ä»¤è¡Œæ¥å£æ¨¡å—
è´Ÿè´£å¤„ç†ç”¨æˆ·è¾“å…¥å’Œè¾“å‡º
"""

import os
from pathlib import Path

class CompilerGeneratorCLI:
    """ç¼–è¯‘å™¨ç”Ÿæˆå™¨å‘½ä»¤è¡Œæ¥å£"""
    
    def __init__(self):
        self.verbose = True
    
    def generate_compiler(self, lexer_rules_path, grammar_rules_path, output_path):
        """
        ç”Ÿæˆç¼–è¯‘å™¨çš„ä¸»æ–¹æ³•
        
        å‚æ•°:
            lexer_rules_path: è¯æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„
            grammar_rules_path: è¯­æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºç¼–è¯‘å™¨æ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹ç”Ÿæˆç¼–è¯‘å™¨...")
        print(f"ğŸ“– è¯æ³•è§„åˆ™: {lexer_rules_path}")
        print(f"ğŸ“– è¯­æ³•è§„åˆ™: {grammar_rules_path}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(lexer_rules_path):
            print(f"âŒ é”™è¯¯: è¯æ³•è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {lexer_rules_path}")
            return
        
        if not os.path.exists(grammar_rules_path):
            print(f"âŒ é”™è¯¯: è¯­æ³•è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {grammar_rules_path}")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # è§£æè§„åˆ™æ–‡ä»¶
            from frontend.rule_parser import RuleParser
            rule_parser = RuleParser()
            
            lexer_rules = rule_parser.parse_lexer_rules(lexer_rules_path)
            grammar_rules = rule_parser.parse_grammar_rules(grammar_rules_path)
            
            print(f"âœ… æˆåŠŸè§£æ {len(lexer_rules)} æ¡è¯æ³•è§„åˆ™")
            print(f"âœ… æˆåŠŸè§£æ {len(grammar_rules)} æ¡è¯­æ³•è§„åˆ™")
            
            # ç”Ÿæˆç¼–è¯‘å™¨
            from compiler_generator.lexer_generator import LexerGenerator
            from compiler_generator.parser_generator import ParserGenerator
            from compiler_generator.code_generator import CodeGenerator
            
            # ç”Ÿæˆè¯æ³•åˆ†æå™¨
            lexer_gen = LexerGenerator(lexer_rules)
            lexer_code = lexer_gen.generate_lexer()
            
            # ç”Ÿæˆè¯­æ³•åˆ†æå™¨
            parser_gen = ParserGenerator(grammar_rules)
            parser_code = parser_gen.generate_parser()
            
            # ç»„åˆç”Ÿæˆå®Œæ•´ç¼–è¯‘å™¨
            code_gen = CodeGenerator()
            full_compiler_code = code_gen.generate_compiler_code(lexer_code, parser_code)
            
            # å†™å…¥è¾“å‡ºæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(full_compiler_code)
            
            print(f"âœ… ç¼–è¯‘å™¨ç”ŸæˆæˆåŠŸ: {output_path}")
            print("\nğŸ“‹ ä½¿ç”¨è¯´æ˜:")
            print(f"   ç¼–è¯‘æºä»£ç : python {output_path} compile --input <source_file> --output <output_file>")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç¼–è¯‘å™¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
```

### 2.3 è§„åˆ™æ–‡ä»¶è§£æå™¨ (src/frontend/rule_parser.py)

```python
"""
è§„åˆ™æ–‡ä»¶è§£æå™¨
è´Ÿè´£è§£æè¯æ³•è§„åˆ™å’Œè¯­æ³•è§„åˆ™æ–‡ä»¶
"""

import re
from typing import List, Dict, Any

class RuleParser:
    """è§„åˆ™æ–‡ä»¶è§£æå™¨"""
    
    def parse_lexer_rules(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£æè¯æ³•è§„åˆ™æ–‡ä»¶
        
        å‚æ•°:
            file_path: è¯æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            è¯æ³•è§„åˆ™åˆ—è¡¨
        """
        rules = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                
                # è§£æè§„åˆ™: TOKEN_NAME : REGEX_PATTERN
                if ':' not in line:
                    print(f"âš ï¸  è­¦å‘Šç¬¬{line_num}è¡Œ: æ— æ•ˆçš„è¯æ³•è§„åˆ™æ ¼å¼: {line}")
                    continue
                
                token_name, regex_pattern = line.split(':', 1)
                token_name = token_name.strip()
                regex_pattern = regex_pattern.strip()
                
                rules.append({
                    'name': token_name,
                    'pattern': regex_pattern,
                    'line': line_num
                })
        
        return rules
    
    def parse_grammar_rules(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£æè¯­æ³•è§„åˆ™æ–‡ä»¶
        
        å‚æ•°:
            file_path: è¯­æ³•è§„åˆ™æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            è¯­æ³•è§„åˆ™åˆ—è¡¨
        """
        rules = []
        current_rule = None
        
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            i += 1
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith('#'):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„äº§ç”Ÿå¼
            if ':' in line and not line.startswith(' '):
                if current_rule:
                    rules.append(current_rule)
                
                # è§£æå·¦éƒ¨å’Œéç»ˆç»“ç¬¦
                left_part, right_part = line.split(':', 1)
                non_terminal = left_part.strip()
                
                current_rule = {
                    'non_terminal': non_terminal,
                    'productions': [],
                    'line': i
                }
                
                # å¤„ç†å³éƒ¨çš„ç¬¬ä¸€ä¸ªé€‰æ‹©
                right_part = right_part.strip()
                if right_part:
                    self._parse_production(current_rule, right_part, i)
            
            # ç»§ç»­å½“å‰äº§ç”Ÿå¼çš„å…¶ä»–é€‰æ‹©
            elif line.startswith('|') and current_rule:
                right_part = line[1:].strip()
                self._parse_production(current_rule, right_part, i)
            
            # è¯­ä¹‰åŠ¨ä½œ
            elif line.startswith('{') and current_rule:
                action = line
                # å¦‚æœåŠ¨ä½œè·¨è¶Šå¤šè¡Œ
                while '}' not in line and i < len(lines):
                    line = lines[i].strip()
                    i += 1
                    action += '\n' + line
                
                # æ·»åŠ åˆ°æœ€åä¸€ä¸ªäº§ç”Ÿå¼
                if current_rule['productions']:
                    current_rule['productions'][-1]['action'] = action
            
            else:
                print(f"âš ï¸  è­¦å‘Šç¬¬{i}è¡Œ: æ— æ³•è§£æçš„è¯­æ³•è§„åˆ™: {line}")
        
        # æ·»åŠ æœ€åä¸€ä¸ªè§„åˆ™
        if current_rule:
            rules.append(current_rule)
        
        return rules
    
    def _parse_production(self, rule: Dict[str, Any], right_part: str, line_num: int):
        """
        è§£æå•ä¸ªäº§ç”Ÿå¼
        
        å‚æ•°:
            rule: å½“å‰è§„åˆ™å­—å…¸
            right_part: äº§ç”Ÿå¼å³éƒ¨å­—ç¬¦ä¸²
            line_num: è¡Œå·
        """
        # ç®€å•çš„åˆ†è¯ï¼Œå®é™…å®ç°åº”è¯¥æ›´å¤æ‚
        symbols = []
        parts = right_part.split()
        
        for part in parts:
            if part in ['|', ':', '{', '}']:
                continue
            symbols.append({
                'type': 'TERMINAL' if part.isupper() else 'NON_TERMINAL',
                'value': part
            })
        
        rule['productions'].append({
            'symbols': symbols,
            'action': None,
            'line': line_num
        })
```

### 2.4 è¯æ³•åˆ†æå™¨ç”Ÿæˆå™¨ (src/compiler_generator/lexer_generator.py)

```python
"""
è¯æ³•åˆ†æå™¨ç”Ÿæˆå™¨ - åŒå­¦Aè´Ÿè´£å®ç°
æ ¹æ®è¯æ³•è§„åˆ™ç”Ÿæˆè¯æ³•åˆ†æå™¨
"""

import re
from typing import List, Dict, Any

class LexerGenerator:
    """è¯æ³•åˆ†æå™¨ç”Ÿæˆå™¨"""
    
    def __init__(self, lexer_rules: List[Dict[str, Any]]):
        self.lexer_rules = lexer_rules
    
    def generate_lexer(self) -> str:
        """
        ç”Ÿæˆè¯æ³•åˆ†æå™¨ä»£ç 
        
        è¿”å›:
            è¯æ³•åˆ†æå™¨çš„Pythonä»£ç å­—ç¬¦ä¸²
        """
        lexer_code = '''
# =============================================================================
# è‡ªåŠ¨ç”Ÿæˆçš„è¯æ³•åˆ†æå™¨
# æ ¹æ®è¯æ³•è§„åˆ™è‡ªåŠ¨ç”Ÿæˆ
# =============================================================================

import re
from typing import List, Dict, Any

class GeneratedLexer:
    """ç”Ÿæˆçš„è¯æ³•åˆ†æå™¨"""
    
    def __init__(self, input_text: str = None):
        """
        åˆå§‹åŒ–è¯æ³•åˆ†æå™¨
        
        å‚æ•°:
            input_text: è¾“å…¥çš„æºä»£ç å­—ç¬¦ä¸²
        """
        self.input_text = input_text
        self.tokens = []
        self.current_index = 0
        self.line = 1
        self.column = 1
        
        # å®šä¹‰tokenæ¨¡å¼
        self.token_patterns = [
'''
        
        # æ·»åŠ tokenæ¨¡å¼
        for rule in self.lexer_rules:
            token_name = rule['name']
            pattern = rule['pattern']
            
            # è·³è¿‡ç©ºç™½å­—ç¬¦çš„tokenç”Ÿæˆ
            if token_name != 'WHITESPACE':
                lexer_code += f"            ('{token_name}', r'{pattern}'),\n"
        
        lexer_code += '''        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.patterns = [(name, re.compile(pattern)) for name, pattern in self.token_patterns]
    
    def tokenize(self, input_text: str) -> List[Dict[str, Any]]:
        """
        å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—
        
        å‚æ•°:
            input_text: è¾“å…¥çš„æºä»£ç å­—ç¬¦ä¸²
            
        è¿”å›:
            tokenåˆ—è¡¨
        """
        self.input_text = input_text
        self.tokens = []
        self.current_index = 0
        self.line = 1
        self.column = 1
        
        position = 0
        
        while position < len(input_text):
            match = None
            matched_name = None
            matched_value = None
            
            # å°è¯•åŒ¹é…æ‰€æœ‰tokenæ¨¡å¼
            for token_name, pattern in self.patterns:
                regex_match = pattern.match(input_text, position)
                if regex_match:
                    # é€‰æ‹©æœ€é•¿çš„åŒ¹é…
                    if match is None or regex_match.end() > match.end():
                        match = regex_match
                        matched_name = token_name
                        matched_value = regex_match.group()
            
            if match:
                # å¤„ç†åŒ¹é…çš„token
                start, end = match.span()
                
                # æ›´æ–°è¡Œå·å’Œåˆ—å·
                matched_text = input_text[start:end]
                newlines = matched_text.count('\\n')
                if newlines > 0:
                    self.line += newlines
                    self.column = 1
                else:
                    self.column += (end - start)
                
                # æ·»åŠ tokenåˆ°åˆ—è¡¨ï¼ˆè·³è¿‡ç©ºç™½å­—ç¬¦ï¼‰
                if matched_name != 'WHITESPACE':
                    token = {
                        'type': matched_name,
                        'value': matched_value,
                        'line': self.line,
                        'column': self.column - (end - start) if newlines == 0 else 1
                    }
                    self.tokens.append(token)
                
                position = end
            else:
                # æ²¡æœ‰åŒ¹é…çš„æ¨¡å¼ï¼ŒæŠ¥å‘Šé”™è¯¯
                raise SyntaxError(
                    f"è¯æ³•é”™è¯¯ç¬¬{self.line}è¡Œç¬¬{self.column}åˆ—: "
                    f"æ— æ³•è¯†åˆ«çš„å­—ç¬¦ '{input_text[position]}'"
                )
        
        # æ·»åŠ æ–‡ä»¶ç»“æŸæ ‡è®°
        self.tokens.append({
            'type': 'EOF',
            'value': None,
            'line': self.line,
            'column': self.column
        })
        
        return self.tokens
    
    def get_next_token(self) -> Dict[str, Any]:
        """
        è·å–ä¸‹ä¸€ä¸ªtoken
        
        è¿”å›:
            tokenå­—å…¸
        """
        if self.current_index < len(self.tokens):
            token = self.tokens[self.current_index]
            self.current_index += 1
            return token
        return {'type': 'EOF', 'value': None, 'line': self.line, 'column': self.column}
    
    def peek_token(self) -> Dict[str, Any]:
        """
        é¢„è§ˆä¸‹ä¸€ä¸ªtokenä½†ä¸æ¶ˆè€—å®ƒ
        
        è¿”å›:
            ä¸‹ä¸€ä¸ªtokençš„å­—å…¸
        """
        if self.current_index < len(self.tokens):
            return self.tokens[self.current_index]
        return {'type': 'EOF', 'value': None, 'line': self.line, 'column': self.column}
'''
        
        return lexer_code
```

### 2.5 è¯­æ³•åˆ†æå™¨ç”Ÿæˆå™¨ (src/compiler_generator/parser_generator.py)

```python
"""
è¯­æ³•åˆ†æå™¨ç”Ÿæˆå™¨ - åŒå­¦Bè´Ÿè´£å®ç°
æ ¹æ®è¯­æ³•è§„åˆ™ç”Ÿæˆè¯­æ³•åˆ†æå™¨ï¼ˆä½¿ç”¨è¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼‰
"""

from typing import List, Dict, Any

class ParserGenerator:
    """è¯­æ³•åˆ†æå™¨ç”Ÿæˆå™¨"""
    
    def __init__(self, grammar_rules: List[Dict[str, Any]]):
        self.grammar_rules = grammar_rules
    
    def generate_parser(self) -> str:
        """
        ç”Ÿæˆè¯­æ³•åˆ†æå™¨ä»£ç 
        
        è¿”å›:
            è¯­æ³•åˆ†æå™¨çš„Pythonä»£ç å­—ç¬¦ä¸²
        """
        parser_code = '''
# =============================================================================
# è‡ªåŠ¨ç”Ÿæˆçš„è¯­æ³•åˆ†æå™¨
# æ ¹æ®è¯­æ³•è§„åˆ™è‡ªåŠ¨ç”Ÿæˆ
# ä½¿ç”¨é€’å½’ä¸‹é™åˆ†æå’Œè¯­æ³•åˆ¶å¯¼ç¿»è¯‘
# =============================================================================

from typing import List, Dict, Any

class GeneratedParser:
    """ç”Ÿæˆçš„è¯­æ³•åˆ†æå™¨"""
    
    def __init__(self):
        self.temp_counter = 0
        self.label_counter = 0
        self.three_address_code = []
        self.symbol_table = {}
        self.lexer = None
        self.current_token = None
    
    def new_temp(self) -> str:
        """ç”Ÿæˆæ–°çš„ä¸´æ—¶å˜é‡å"""
        self.temp_counter += 1
        return f't{self.temp_counter}'
    
    def new_label(self) -> str:
        """ç”Ÿæˆæ–°çš„æ ‡ç­¾å"""
        self.label_counter += 1
        return f'L{self.label_counter}'
    
    def emit(self, code: str):
        """ç”Ÿæˆä¸‰åœ°å€ç æŒ‡ä»¤"""
        self.three_address_code.append(code)
        print(f"ç”Ÿæˆä»£ç : {code}")
    
    def match(self, expected_type: str):
        """åŒ¹é…å½“å‰tokençš„ç±»å‹"""
        if self.current_token['type'] == expected_type:
            value = self.current_token['value']
            self.current_token = self.lexer.get_next_token()
            return value
        else:
            raise SyntaxError(
                f"è¯­æ³•é”™è¯¯ç¬¬{self.current_token['line']}è¡Œç¬¬{self.current_token['column']}åˆ—: "
                f"æœŸæœ› {expected_type}ï¼Œä½†å¾—åˆ° {self.current_token['type']}"
            )
    
    def parse(self, lexer) -> List[str]:
        """
        æ‰§è¡Œè¯­æ³•åˆ†æå¹¶ç”Ÿæˆä¸­é—´ä»£ç 
        
        å‚æ•°:
            lexer: è¯æ³•åˆ†æå™¨å®ä¾‹
            
        è¿”å›:
            ä¸‰åœ°å€ç åˆ—è¡¨
        """
        self.lexer = lexer
        self.current_token = lexer.get_next_token()
        self.three_address_code = []
        self.temp_counter = 0
        self.label_counter = 0
        self.symbol_table = {}
        
        # ä»å¼€å§‹ç¬¦å·å¼€å§‹è§£æ
        self.parse_program()
        
        return self.three_address_code
'''
        
        # ä¸ºæ¯ä¸ªéç»ˆç»“ç¬¦ç”Ÿæˆè§£ææ–¹æ³•
        for rule in self.grammar_rules:
            non_terminal = rule['non_terminal']
            productions = rule['productions']
            
            parser_code += f'''
    def parse_{non_terminal}(self):
        """è§£æ {non_terminal}"""
'''
            
            # ä¸ºæ¯ä¸ªäº§ç”Ÿå¼ç”Ÿæˆä»£ç 
            if len(productions) == 1:
                # å•ä¸ªäº§ç”Ÿå¼
                production = productions[0]
                parser_code += self._generate_production_code(production, non_terminal)
            else:
                # å¤šä¸ªäº§ç”Ÿå¼ï¼Œéœ€è¦æ ¹æ®å‰ç»tokené€‰æ‹©
                parser_code += '        # æ ¹æ®å‰ç»tokené€‰æ‹©äº§ç”Ÿå¼\\n'
                parser_code += f'        next_token_type = self.current_token["type"]\\n'
                
                for i, production in enumerate(productions):
                    if i == 0:
                        parser_code += f'        if '
                    else:
                        parser_code += f'        elif '
                    
                    # ç”Ÿæˆæ¡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
                    first_symbol = production['symbols'][0] if production['symbols'] else None
                    if first_symbol and first_symbol['type'] == 'TERMINAL':
                        parser_code += f'next_token_type == "{first_symbol["value"]}":\\n'
                    else:
                        parser_code += f'True:  # é»˜è®¤é€‰æ‹©\\n'
                    
                    parser_code += self._generate_production_code(production, non_terminal, indent=12)
                
                parser_code += '        else:\\n'
                parser_code += '            raise SyntaxError(f"æ„å¤–çš„token: {next_token_type}")\\n'
        
        return parser_code
    
    def _generate_production_code(self, production: Dict[str, Any], non_terminal: str, indent: int = 8) -> str:
        """
        ä¸ºå•ä¸ªäº§ç”Ÿå¼ç”Ÿæˆè§£æä»£ç 
        
        å‚æ•°:
            production: äº§ç”Ÿå¼å­—å…¸
            non_terminal: éç»ˆç»“ç¬¦åç§°
            indent: ç¼©è¿›ç©ºæ ¼æ•°
            
        è¿”å›:
            ç”Ÿæˆçš„ä»£ç å­—ç¬¦ä¸²
        """
        indent_str = ' ' * indent
        code = ''
        
        # å¤„ç†äº§ç”Ÿå¼ä¸­çš„ç¬¦å·
        for symbol in production['symbols']:
            if symbol['type'] == 'TERMINAL':
                code += f'{indent_str}self.match("{symbol["value"]}")\\n'
            else:  # NON_TERMINAL
                code += f'{indent_str}result_{symbol["value"]} = self.parse_{symbol["value"]}()\\n'
        
        # å¤„ç†è¯­ä¹‰åŠ¨ä½œ
        if production['action']:
            # è¿™é‡Œåº”è¯¥è§£æè¯­ä¹‰åŠ¨ä½œå¹¶ç”Ÿæˆç›¸åº”çš„ä»£ç 
            # ç®€åŒ–ç‰ˆï¼šç›´æ¥åµŒå…¥åŠ¨ä½œä»£ç 
            action_code = production['action'].strip('{}').strip()
            code += f'{indent_str}# è¯­ä¹‰åŠ¨ä½œ\\n'
            code += f'{indent_str}{action_code}\\n'
        else:
            code += f'{indent_str}# æ— è¯­ä¹‰åŠ¨ä½œ\\n'
            # é»˜è®¤è¿”å›æœ€åä¸€ä¸ªç¬¦å·çš„ç»“æœ
            if production['symbols']:
                last_symbol = production['symbols'][-1]
                if last_symbol['type'] == 'NON_TERMINAL':
                    code += f'{indent_str}return result_{last_symbol["value"]}\\n'
                else:
                    code += f'{indent_str}return None\\n'
            else:
                code += f'{indent_str}return None\\n'
        
        return code
```

### 2.6 ä»£ç ç”Ÿæˆå™¨ (src/compiler_generator/code_generator.py)

```python
"""
ä»£ç ç”Ÿæˆå™¨ - åŒå­¦Cè´Ÿè´£å®ç°
å°†è¯æ³•åˆ†æå™¨å’Œè¯­æ³•åˆ†æå™¨ç»„åˆæˆå®Œæ•´çš„ç¼–è¯‘å™¨
"""

class CodeGenerator:
    """ä»£ç ç”Ÿæˆå™¨"""
    
    def generate_compiler_code(self, lexer_code: str, parser_code: str) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ç¼–è¯‘å™¨ä»£ç 
        
        å‚æ•°:
            lexer_code: è¯æ³•åˆ†æå™¨ä»£ç 
            parser_code: è¯­æ³•åˆ†æå™¨ä»£ç 
            
        è¿”å›:
            å®Œæ•´çš„ç¼–è¯‘å™¨ä»£ç 
        """
        compiler_code = f'''#!/usr/bin/env python3
# =============================================================================
# è‡ªåŠ¨ç”Ÿæˆçš„ç¼–è¯‘å™¨
# æ ¹æ®è¯æ³•è§„åˆ™å’Œè¯­æ³•è§„åˆ™è‡ªåŠ¨ç”Ÿæˆ
# ç”Ÿæˆæ—¶é—´: {self._get_current_time()}
# =============================================================================

import argparse
import sys
{lexer_code}

{parser_code}

# =============================================================================
# ç”Ÿæˆçš„ç¼–è¯‘å™¨ä¸»ç±»
# =============================================================================

class GeneratedCompiler:
    """ç”Ÿæˆçš„ç¼–è¯‘å™¨"""
    
    def __init__(self):
        self.lexer = GeneratedLexer()
        self.parser = GeneratedParser()
    
    def compile(self, source_code: str) -> List[str]:
        """
        ç¼–è¯‘æºä»£ç 
        
        å‚æ•°:
            source_code: æºä»£ç å­—ç¬¦ä¸²
            
        è¿”å›:
            ä¸‰åœ°å€ç åˆ—è¡¨
        """
        try:
            # è¯æ³•åˆ†æ
            tokens = self.lexer.tokenize(source_code)
            
            # è¯­æ³•åˆ†æå’Œä¸­é—´ä»£ç ç”Ÿæˆ
            three_address_code = self.parser.parse(self.lexer)
            
            return three_address_code
            
        except Exception as e:
            print(f"ç¼–è¯‘é”™è¯¯: {{e}}")
            raise
    
    def compile_file(self, input_file: str, output_file: str):
        """
        ç¼–è¯‘æ–‡ä»¶
        
        å‚æ•°:
            input_file: è¾“å…¥æºä»£ç æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºä¸‰åœ°å€ç æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ”¨ å¼€å§‹ç¼–è¯‘: {{input_file}}")
        
        # è¯»å–æºä»£ç 
        with open(input_file, 'r', encoding='utf-8') as f:
            source_code = f.read()
        
        # ç¼–è¯‘
        three_address_code = self.compile(source_code)
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            for code in three_address_code:
                f.write(code + '\\n')
        
        print(f"âœ… ç¼–è¯‘å®Œæˆ: {{output_file}}")
        print(f"ğŸ“Š ç”Ÿæˆ {{len(three_address_code)}} æ¡ä¸‰åœ°å€ç ")

# =============================================================================
# ä¸»ç¨‹åºå…¥å£ï¼ˆå½“ç›´æ¥è¿è¡Œç”Ÿæˆçš„ç¼–è¯‘å™¨æ—¶ï¼‰
# =============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆçš„ç¼–è¯‘å™¨")
    parser.add_argument("command", help="å‘½ä»¤ (compile)")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥æºä»£ç æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºä¸‰åœ°å€ç æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.command == "compile":
        compiler = GeneratedCompiler()
        compiler.compile_file(args.input, args.output)
    else:
        print(f"é”™è¯¯: æœªçŸ¥å‘½ä»¤ {{args.command}}")
        parser.print_help()
'''
        
        return compiler_code
    
    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

## 3. æµ‹è¯•æ–‡ä»¶

### 3.1 æµ‹è¯•ç”¨ä¾‹ (tests/test_integration.py)

```python
"""
é›†æˆæµ‹è¯•ç”¨ä¾‹
æµ‹è¯•å®Œæ•´çš„ç¼–è¯‘å™¨ç”Ÿæˆå’Œä½¿ç”¨æµç¨‹
"""

import os
import tempfile
import subprocess
from pathlib import Path

def test_compiler_generation():
    """æµ‹è¯•ç¼–è¯‘å™¨ç”Ÿæˆ"""
    print("ğŸ§ª æµ‹è¯•ç¼–è¯‘å™¨ç”Ÿæˆ...")
    
    # ä½¿ç”¨ç¤ºä¾‹è§„åˆ™ç”Ÿæˆç¼–è¯‘å™¨
    lexer_rules = "examples/simple_expr/lexer_rules.txt"
    grammar_rules = "examples/simple_expr/grammar_rules.txt"
    output_compiler = "generated/test_compiler.py"
    
    # è¿è¡Œç¼–è¯‘å™¨ç”Ÿæˆå™¨
    cmd = [
        "python", "main.py", "generate",
        "--lexer", lexer_rules,
        "--grammar", grammar_rules,
        "--output", output_compiler
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ ç¼–è¯‘å™¨ç”Ÿæˆå¤±è´¥: {result.stderr}")
        return False
    
    print("âœ… ç¼–è¯‘å™¨ç”ŸæˆæˆåŠŸ")
    
    # æµ‹è¯•ç”Ÿæˆçš„ç¼–è¯‘å™¨
    return test_generated_compiler(output_compiler)

def test_generated_compiler(compiler_path):
    """æµ‹è¯•ç”Ÿæˆçš„ç¼–è¯‘å™¨"""
    print("ğŸ§ª æµ‹è¯•ç”Ÿæˆçš„ç¼–è¯‘å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æºä»£ç 
    test_source = """
x = 2 + 3 * 4;
y = (5 + 6) * 2;
print(x);
print(y);
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write(test_source)
        input_file = f.name
    
    output_file = "generated/test_output.tac"
    
    try:
        # è¿è¡Œç”Ÿæˆçš„ç¼–è¯‘å™¨
        cmd = [
            "python", compiler_path, "compile",
            "--input", input_file,
            "--output", output_file
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âŒ ç¼–è¯‘å¤±è´¥: {result.stderr}")
            return False
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                tac = f.read()
            print(f"âœ… ç¼–è¯‘æˆåŠŸï¼Œç”Ÿæˆä¸‰åœ°å€ç :")
            print(tac)
            return True
        else:
            print("âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(input_file):
            os.unlink(input_file)

if __name__ == "__main__":
    print("=" * 60)
    print("           ç¼–è¯‘å™¨é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    success = test_compiler_generation()
    
    print("\\n" + "=" * 60)
    if success:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥!")
    print("=" * 60)
```

## 4. å®é™…ä½¿ç”¨ç¤ºä¾‹

### 4.1 ç¼–è¯‘å‘½ä»¤

```bash
# åŸºæœ¬ç”¨æ³•
python main.py compile <lexer_rules> <grammar_rules> <source_file> -o <output_file>

# å…·ä½“ä¾‹å­
python main.py compile \
  examples/simple_expr/lexer_rules.txt \
  examples/simple_expr/grammar_rules.txt \
  examples/sample.src \
  -o generated/output.tac
```

### 4.2 ç¤ºä¾‹æºä»£ç  (examples/sample.src)

```
// ç®€å•è¡¨è¾¾å¼è¯­è¨€ç¤ºä¾‹ç¨‹åº
x = 10 ;
y = 20 ;
print(x + y);
```

### 4.3 ç”Ÿæˆçš„ä¸‰åœ°å€ç  (generated/output.tac)

```
x = 10
y = 20
t1 = x + y
print(t1)
```

### 4.4 è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest tests/test_lexer.py -v
python -m pytest tests/test_parser.py -v
python -m pytest tests/test_integration.py -v
```

## 5. é¡¹ç›®æˆå‘˜åˆ†å·¥

| æ¨¡å— | è´Ÿè´£äºº | æ–‡ä»¶ | ä¸»è¦èŒè´£ |
|------|--------|------|---------|
| è¯æ³•åˆ†æ | åŒå­¦ A | `src/compiler_generator/lexer_generator.py` | è¯æ³•åˆ†æå™¨ç”Ÿæˆå™¨å®ç° |
| è¯­æ³•åˆ†æ | åŒå­¦ B | `src/compiler_generator/parser_generator.py` | è¯­æ³•åˆ†æå™¨ç”Ÿæˆå™¨å®ç° |
| ä»£ç ç”Ÿæˆ | åŒå­¦ C | `src/compiler_generator/code_generator.py` | ä»£ç ç”Ÿæˆå™¨å®ç° |
| å‰ç«¯æ¥å£ | å…¨ä½“ | `src/frontend/` | CLI å’Œè§„åˆ™è§£æå™¨ |
| å·¥å…·æ¨¡å— | å…¨ä½“ | `src/utils/` | é”™è¯¯å¤„ç†å’Œæ—¥å¿— |
| æµ‹è¯• | å…¨ä½“ | `tests/` | å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯• |

## 6. å…³é”®æŠ€æœ¯ç‚¹

### 6.1 è¯æ³•åˆ†æ
- **æ­£åˆ™è¡¨è¾¾å¼ç¼–è¯‘**ï¼šä½¿ç”¨ Python `re` æ¨¡å—ç¼–è¯‘è§„åˆ™
- **ä½ç½®è¿½è¸ª**ï¼šè®°å½•æ¯ä¸ª Token çš„è¡Œåˆ—å·
- **é”™è¯¯å¤„ç†**ï¼šæ— æ³•è¯†åˆ«å­—ç¬¦æ—¶æŠ¥å‘Šç²¾ç¡®ä½ç½®

### 6.2 è¯­æ³•åˆ†æ
- **é€’å½’ä¸‹é™è§£æ**ï¼šä¸ºæ¯ä¸ªéç»ˆç»“ç¬¦ç”Ÿæˆè§£ææ–¹æ³•
- **å›æº¯æœºåˆ¶**ï¼šäº§ç”Ÿå¼å¤±è´¥æ—¶å›æº¯å¹¶å°è¯•å…¶ä»–é€‰æ‹©
- **AST æ„å»º**ï¼šé€’å½’è¿‡ç¨‹ä¸­æ„å»ºè¯­æ³•æ ‘

### 6.3 ä»£ç ç”Ÿæˆ
- **ä¸´æ—¶å˜é‡ç®¡ç†**ï¼šç”Ÿæˆ t1, t2, ... ç”¨äºä¸­é—´å€¼
- **ä¸‰åœ°å€ç å‘å°„**ï¼šé€’å½’ç”ŸæˆæŒ‡ä»¤åºåˆ—
- **ç¬¦å·è¡¨ç»´æŠ¤**ï¼šè®°å½•å˜é‡å’Œå®ƒä»¬çš„åˆ†é…

### 6.4 é€’å½’ä¸‹é™è§£æçš„æ”¹è¿›
- **é¿å…å³é€’å½’**ï¼šä½¿ç”¨ AddOpã€MulOp ç­‰è¾…åŠ©éç»ˆç»“ç¬¦
- **äº§ç”Ÿå¼æ’åº**ï¼šé€’å½’äº§ç”Ÿå¼æ”¾åœ¨å‰é¢ä¾¿äºå›æº¯
- **æ™ºèƒ½é€‰æ‹©**ï¼šæ ¹æ®å‰ç»ç¬¦å·é€‰æ‹©æ­£ç¡®çš„äº§ç”Ÿå¼

## 7. æµ‹è¯•ç»Ÿè®¡

- **æ€»æµ‹è¯•æ•°**ï¼š14 ä¸ª
- **è¯æ³•æµ‹è¯•**ï¼š6 ä¸ªï¼ˆè§„åˆ™æ·»åŠ ã€æ„å»ºã€ç®€å•åˆ†æã€ç©ºç™½ç¬¦å¤„ç†ã€é”™è¯¯å¤„ç†ã€ä½ç½®è¿½è¸ªï¼‰
- **è¯­æ³•æµ‹è¯•**ï¼š6 ä¸ªï¼ˆåˆ›å»ºè§£æå™¨ã€æ·»åŠ äº§ç”Ÿå¼ã€ç®€å•è§£æã€å¤šé€‰æ‹©äº§ç”Ÿå¼ã€é”™è¯¯å¤„ç†ã€AST èŠ‚ç‚¹åˆ›å»ºï¼‰
- **é›†æˆæµ‹è¯•**ï¼š2 ä¸ªï¼ˆç«¯åˆ°ç«¯å®Œæ•´ç¼–è¯‘ã€è§„åˆ™è§£æéªŒè¯ï¼‰
- **æµ‹è¯•é€šè¿‡ç‡**ï¼š100% (14/14)

## 8. é¡¹ç›®ç‰¹ç‚¹

âœ… **å®Œæ•´æ€§**ï¼šä»è§„åˆ™è§£æåˆ°ä»£ç ç”Ÿæˆçš„å®Œæ•´ç¼–è¯‘æµç¨‹
âœ… **è§„èŒƒæ€§**ï¼šä¸¥æ ¼æŒ‰ç…§ MVP ç»“æ„è¯´æ˜å®ç°
âœ… **å¯è¯»æ€§**ï¼šè¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œæ–‡æ¡£
âœ… **å¯æµ‹è¯•æ€§**ï¼šå®Œå–„çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
âœ… **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ‰©å±•æ”¯æŒæ–°çš„è¯­è¨€ç‰¹æ€§

## 9. ä¿®å¤æ—¥å¿—

### ä¿®å¤ 1ï¼šè¯­æ³•è§„åˆ™çš„äº§ç”Ÿå¼é¡ºåº
- **é—®é¢˜**ï¼š`StmtList -> Stmt | Stmt StmtList` å¯¼è‡´å¤šè¯­å¥æ— æ³•è§£æ
- **ä¿®å¤**ï¼šæ”¹ä¸º `StmtList -> Stmt StmtList | Stmt`
- **åŸå› **ï¼šé€’å½’äº§ç”Ÿå¼æ”¾åœ¨å‰é¢èƒ½æ­£ç¡®å›æº¯å’Œé€‰æ‹©

### ä¿®å¤ 2ï¼šè¡¨è¾¾å¼å³é€’å½’
- **é—®é¢˜**ï¼š`Expr -> Term | Term 'PLUS' Expr` å¯¼è‡´å³é€’å½’é—®é¢˜
- **ä¿®å¤**ï¼šæ”¹ä¸ºä½¿ç”¨ `AddOp` å’Œ `MulOp` è¾…åŠ©éç»ˆç»“ç¬¦
- **åŸå› **ï¼šé¿å…æ— é™é€’å½’ï¼Œä½¿å›æº¯æ›´å¯æ§

### ä¿®å¤ 3ï¼šè§£æå™¨ç»ˆç»“ç¬¦è¯†åˆ«
- **é—®é¢˜**ï¼šå¸¦å¼•å·çš„ç»ˆç»“ç¬¦ `'ID'` æ— æ³•æ­£ç¡®åŒ¹é…
- **ä¿®å¤**ï¼šåœ¨ `parse_symbol` ä¸­åŒæ—¶æ”¯æŒå¸¦å¼•å·å’Œä¸å¸¦å¼•å·å½¢å¼
- **åŸå› **ï¼šå…¼å®¹è§„åˆ™æ–‡ä»¶æ ¼å¼å’Œå•å…ƒæµ‹è¯•

### ä¿®å¤ 4ï¼šä»£ç ç”Ÿæˆå™¨ AST éå†
- **é—®é¢˜**ï¼šæ–°çš„è¯­æ³•æ ‘ç»“æ„ä¸­ç»ˆç»“ç¬¦åä»¥å¼•å·æ‹¬èµ·å¯¼è‡´æ— æ³•è¯†åˆ«
- **ä¿®å¤**ï¼šæ›´æ–°ç»ˆç»“ç¬¦è¯†åˆ«æ¡ä»¶ï¼Œæ”¯æŒ `'NUM'`ã€`'ID'` ç­‰æ ¼å¼
- **åŸå› **ï¼šä»£ç ç”Ÿæˆå™¨éœ€è¦æ­£ç¡®éå† AST ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ç±»å‹

### ä¿®å¤ 5ï¼šèµ‹å€¼å’Œ print è¯­å¥ç”Ÿæˆ
- **é—®é¢˜**ï¼šä»£ç ç”Ÿæˆå™¨ä¸èƒ½æ­£ç¡®åŒºåˆ†èµ‹å€¼å’Œ print è¯­å¥
- **ä¿®å¤**ï¼šæ£€æŸ¥ Stmt çš„ç¬¬ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œåˆ†åˆ«å¤„ç† `'ID'` å’Œ `'PRINT'` æƒ…å†µ
- **åŸå› **ï¼šä¸¤ç§è¯­å¥çš„ AST ç»“æ„ä¸åŒï¼Œéœ€è¦ä¸åŒçš„ä»£ç ç”Ÿæˆé€»è¾‘