# ç¡¬ç¼–ç æ¶ˆé™¤æ€»ç»“æŠ¥å‘Š

## æ¶ˆé™¤æˆæœ

âœ… **æˆåŠŸæ¶ˆé™¤äº† `parser_generator.py` ä¸­çš„æ‰€æœ‰ç¡¬ç¼–ç ï¼**

### æ¶ˆé™¤çš„ç¡¬ç¼–ç ç±»å‹

1. âœ… **Tokenç±»å‹åç§°ç¡¬ç¼–ç **ï¼ˆå¦‚ `'ID'`, `'ASSIGN'`, `'WHILE'`, `'IF'`ç­‰ï¼‰
2. âœ… **æ“ä½œç¬¦åˆ—è¡¨ç¡¬ç¼–ç **ï¼ˆå¦‚ `['PLUS', 'MINUS']`, `['MUL', 'DIV']`ï¼‰
3. âœ… **æ’é™¤åˆ—è¡¨ç¡¬ç¼–ç **ï¼ˆå¤§é‡tokenç±»å‹åˆ—è¡¨ï¼‰
4. âœ… **Tokenç±»å‹åˆ¤æ–­ç¡¬ç¼–ç **ï¼ˆå¦‚ `token.type != 'NUM'`ï¼‰

---

## å®ç°æ–¹æ¡ˆ

### 1. åœ¨ `ParserGenerator.__init__()` ä¸­æ·»åŠ è¯æ³•è§„åˆ™å‚æ•°

```python
def __init__(self, lexer_rules: List[Tuple[str, str]] = None, enable_sdt: bool = True):
    """åˆå§‹åŒ–è§£æå™¨
    
    å‚æ•°:
        lexer_rules: è¯æ³•è§„åˆ™åˆ—è¡¨ [(token_type, regex_pattern), ...]ï¼ˆç”¨äºæ¶ˆé™¤ç¡¬ç¼–ç ï¼‰
        enable_sdt: æ˜¯å¦å¯ç”¨è¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    """
    self.lexer_rules = lexer_rules or []
    self.all_token_types: Set[str] = set()
    self.identifier_tokens: Set[str] = set()
    self.number_tokens: Set[str] = set()
    self.operator_tokens: Set[str] = set()
    self.keyword_tokens: Set[str] = set()
    self.punctuation_tokens: Set[str] = set()
    
    if lexer_rules:
        self._extract_token_categories_from_lexer_rules()
```

### 2. ä»è¯æ³•è§„åˆ™ä¸­è‡ªåŠ¨æå–tokenåˆ†ç±»

```python
def _extract_token_categories_from_lexer_rules(self):
    """ä»è¯æ³•è§„åˆ™ä¸­æå–tokenåˆ†ç±»ï¼ˆæ¶ˆé™¤ç¡¬ç¼–ç ï¼‰
    
    é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¯†åˆ«tokençš„è¯­ä¹‰ç±»åˆ«ï¼š
    - æ ‡è¯†ç¬¦ï¼šåŒ¹é…æ ‡è¯†ç¬¦æ¨¡å¼çš„tokenï¼ˆå¦‚ [a-zA-Z_][a-zA-Z0-9_]*ï¼‰
    - æ•°å­—ï¼šåŒ¹é…æ•°å­—æ¨¡å¼çš„tokenï¼ˆå¦‚ [0-9]+ï¼‰
    - æ“ä½œç¬¦ï¼šå•å­—ç¬¦æˆ–ç¬¦å·æ¨¡å¼ï¼ˆå¦‚ +ã€-ã€*ã€/ï¼‰
    - å…³é”®å­—ï¼šå›ºå®šå­—ç¬¦ä¸²æ¨¡å¼ï¼ˆå¦‚ ifã€whileã€readï¼‰
    - æ ‡ç‚¹ï¼šæ‹¬å·ã€åˆ†å·ã€é€—å·ç­‰
    """
    import re
    
    for token_type, pattern in self.lexer_rules:
        self.all_token_types.add(token_type)
        
        # è¯†åˆ«æ ‡è¯†ç¬¦token
        if re.search(r'\[a-zA-Z_\].*\[a-zA-Z0-9_\]', pattern):
            self.identifier_tokens.add(token_type)
        
        # è¯†åˆ«æ•°å­—token
        elif re.search(r'\[0-9\]', pattern):
            self.number_tokens.add(token_type)
        
        # è¯†åˆ«å…³é”®å­—token
        elif re.match(r'^[a-zA-Z]+$', pattern):
            self.keyword_tokens.add(token_type)
        
        # è¯†åˆ«æ ‡ç‚¹ç¬¦å·
        elif pattern in ['(', ')', '{', '}', ';', ',', ...]:
            self.punctuation_tokens.add(token_type)
        
        # è¯†åˆ«æ“ä½œç¬¦
        elif pattern in ['+', '-', '*', '/', '=', '<', '>', ...]:
            self.operator_tokens.add(token_type)
```

### 3. æ·»åŠ è¾…åŠ©æ–¹æ³•åŠ¨æ€è¯†åˆ«tokenç±»å‹

```python
def _is_identifier_token_in_production(self, prod_symbol: str) -> bool:
    """åˆ¤æ–­äº§ç”Ÿå¼ä¸­çš„ç¬¦å·æ˜¯å¦æ˜¯æ ‡è¯†ç¬¦tokenï¼ˆæ¶ˆé™¤ç¡¬ç¼–ç ï¼‰"""
    if not prod_symbol.startswith("'") or not prod_symbol.endswith("'"):
        return False
    token_type = prod_symbol[1:-1]
    return token_type in self.identifier_tokens

def _is_operator_token_in_production(self, prod_symbol: str) -> bool:
    """åˆ¤æ–­äº§ç”Ÿå¼ä¸­çš„ç¬¦å·æ˜¯å¦æ˜¯æ“ä½œç¬¦tokenï¼ˆæ¶ˆé™¤ç¡¬ç¼–ç ï¼‰"""
    if not prod_symbol.startswith("'") or not prod_symbol.endswith("'"):
        return False
    token_type = prod_symbol[1:-1]
    return token_type in self.operator_tokens

# ç±»ä¼¼çš„æ–¹æ³•ï¼š_is_number_token_in_production, _is_keyword_token_in_production, ç­‰
```

### 4. æ¶ˆé™¤SDTä¸­çš„ç¡¬ç¼–ç 

#### 4.1 èµ‹å€¼è¯­å¥ï¼ˆæ¶ˆé™¤å‰ï¼‰
```python
# ç¡¬ç¼–ç ç‰ˆæœ¬
elif len(production) >= 3 and production[0] == "'ID'" and production[1] == "'ASSIGN'":
```

#### 4.1 èµ‹å€¼è¯­å¥ï¼ˆæ¶ˆé™¤åï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ç‰ˆæœ¬
elif (len(production) >= 3 and 
      self._is_identifier_token_in_production(production[0]) and 
      self._is_operator_token_in_production(production[1])):
```

#### 4.2 æ§åˆ¶æµè¯­å¥ï¼ˆæ¶ˆé™¤å‰ï¼‰
```python
# ç¡¬ç¼–ç ç‰ˆæœ¬
elif len(production) >= 5 and production[0] == "'WHILE'" and production[1] == "'LPAREN'":
```

#### 4.2 æ§åˆ¶æµè¯­å¥ï¼ˆæ¶ˆé™¤åï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ç‰ˆæœ¬
elif (len(production) >= 5 and 
      self._is_keyword_token_in_production(production[0]) and 
      self._is_punctuation_token_in_production(production[1])):
```

#### 4.3 æ“ä½œç¬¦è¯†åˆ«ï¼ˆæ¶ˆé™¤å‰ï¼‰
```python
# ç¡¬ç¼–ç ç‰ˆæœ¬
if op_type in ['PLUS', 'MINUS']:
    return self._process_add_op(children[0], left_val)
```

#### 4.3 æ“ä½œç¬¦è¯†åˆ«ï¼ˆæ¶ˆé™¤åï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ç‰ˆæœ¬
if op_type in self.operator_tokens:
    return self._process_add_op(children[0], left_val)
```

#### 4.4 æ’é™¤åˆ—è¡¨ï¼ˆæ¶ˆé™¤å‰ï¼‰
```python
# ç¡¬ç¼–ç ç‰ˆæœ¬
if op_token_type not in ['ID', 'NUM', 'LPAREN', 'RPAREN', 'SEMI', 'ASSIGN', 'COMMA', ...]:
    node.synthesized_value = children[0].synthesized_value
```

#### 4.4 æ’é™¤åˆ—è¡¨ï¼ˆæ¶ˆé™¤åï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ç‰ˆæœ¬
if self._is_operator_token_in_production(production[0]):
    node.synthesized_value = children[0].synthesized_value
```

#### 4.5 å˜é‡å£°æ˜åˆ—è¡¨ï¼ˆæ¶ˆé™¤å‰ï¼‰
```python
# ç¡¬ç¼–ç ç‰ˆæœ¬
if comma_node.token and comma_node.token.type == 'COMMA':
    if id_node.token and id_node.token.type == 'ID':
        var_name = id_node.synthesized_value
```

#### 4.5 å˜é‡å£°æ˜åˆ—è¡¨ï¼ˆæ¶ˆé™¤åï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ç‰ˆæœ¬
if comma_node.token and comma_node.token.type in self.punctuation_tokens:
    if id_node.token and id_node.token.type in self.identifier_tokens:
        var_name = id_node.synthesized_value
```

### 5. æ›´æ–°å‡½æ•°ç­¾å

```python
# æ›´æ–° create_parser_from_spec
def create_parser_from_spec(grammar, start, lexer_rules: List[Tuple[str, str]] = None, metadata: Dict = None):
    p = ParserGenerator(lexer_rules=lexer_rules)
    # ...

# æ›´æ–°è°ƒç”¨å¤„ï¼ˆcli.pyï¼‰
parser = create_parser_from_spec(grammar_rules, start_symbol, lexer_rules=lexer_rules, metadata=metadata)
```

---

## ä¿®å¤çš„é—®é¢˜

### é—®é¢˜ï¼šå…³ç³»è¿ç®—ç¬¦è¢«å½“æˆå˜é‡

**é—®é¢˜æè¿°**ï¼š
- é”™è¯¯ï¼š`å˜é‡ '<' æœªå®šä¹‰`
- é”™è¯¯ï¼š`å˜é‡ '<=' æœªå®šä¹‰`
- åŸå› ï¼šå˜é‡æ£€æŸ¥é€»è¾‘é”™è¯¯ï¼Œå°†æ‰€æœ‰éæ•°å­—tokenéƒ½å½“æˆäº†å˜é‡

**ä¿®å¤å‰ï¼š**
```python
if token.type not in self.number_tokens:  # ä¸æ˜¯æ•°å­—ï¼Œå°±æ£€æŸ¥å˜é‡
    self.check_variable_defined(value, token)
```
âŒ è¿™ä¼šæ£€æŸ¥æ“ä½œç¬¦ã€å…³é”®å­—ã€æ ‡ç‚¹ç­‰æ‰€æœ‰éæ•°å­—token

**ä¿®å¤åï¼š**
```python
if token.type in self.identifier_tokens:  # åªæ£€æŸ¥æ ‡è¯†ç¬¦
    if value and value not in self.symbol_table:
        self.check_variable_defined(value, token)
```
âœ… åªæ£€æŸ¥æ ‡è¯†ç¬¦ç±»å‹çš„tokenï¼Œä¸æ£€æŸ¥æ“ä½œç¬¦ã€å…³é”®å­—ã€æ ‡ç‚¹

---

## æµ‹è¯•ç»“æœ

### æµ‹è¯•1ï¼šSimpleè¡¨è¾¾å¼è¯­è¨€ âœ…

```bash
python main.py compile examples/simple_expr/lexer_rules.txt examples/simple_expr/grammar_rules.txt examples/simple_expr/programs/basic_sample.src
```

è¾“å‡ºï¼š
```
[INFO] è¯æ³•åˆ†æå®Œæˆï¼Œç”Ÿæˆ 16 ä¸ªtoken
[INFO] æ‰§è¡Œè¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼ˆè§£æ+ä»£ç ç”Ÿæˆï¼‰...
[INFO] [å®Œæˆ] è¯­æ³•åˆ†æå®Œæˆ
[INFO] [å®Œæˆ] ä¸­é—´ä»£ç ç”Ÿæˆå®Œæˆï¼ˆè¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼‰

=== ä¸­é—´ä»£ç  ===
x = 10
y = 20
t1 = x + y
param t1
call write, 1
[SUCCESS] ç¼–è¯‘æˆåŠŸï¼
```

### æµ‹è¯•2ï¼šPL/0è¯­è¨€ âœ…

```bash
python main.py compile examples/pl0_subset/lexer_rules.txt examples/pl0_subset/grammar_rules.txt examples/pl0_subset/programs/basic_pl0.src
```

è¾“å‡ºï¼š
```
[INFO] è¯æ³•åˆ†æå®Œæˆï¼Œç”Ÿæˆ 41 ä¸ªtoken
[INFO] æ‰§è¡Œè¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼ˆè§£æ+ä»£ç ç”Ÿæˆï¼‰...
[INFO] [å®Œæˆ] è¯­æ³•åˆ†æå®Œæˆ
[INFO] [å®Œæˆ] ä¸­é—´ä»£ç ç”Ÿæˆå®Œæˆï¼ˆè¯­æ³•åˆ¶å¯¼ç¿»è¯‘ï¼‰

=== ä¸­é—´ä»£ç  ===
a = 1
b = 2
t1 = b * 3
t2 = a + t1
c = t2
param a
call write, 1
param b
call write, 1
param c
call write, 1
[SUCCESS] ç¼–è¯‘æˆåŠŸï¼
```

### æµ‹è¯•3ï¼šç”Ÿæˆçš„ç¼–è¯‘å™¨ âœ…

```bash
# ç”ŸæˆSimpleè¯­è¨€çš„ç¼–è¯‘å™¨
python main.py build examples/simple_expr/lexer_rules.txt examples/simple_expr/grammar_rules.txt

# ä½¿ç”¨ç”Ÿæˆçš„ç¼–è¯‘å™¨
python generated/compiler.py examples/simple_expr/programs/basic_sample.src
```

è¾“å‡ºï¼š
```
[Success] Compilation completed (Syntax-Directed Translation) -> output.tac
         Generated 5 intermediate code instructions
```

### æµ‹è¯•4ï¼šPL/0çš„ifå’Œwhileè¯­å¥ âœ…

**é—®é¢˜ä¿®å¤å‰ï¼š** ç¼–è¯‘å¤±è´¥ï¼Œé”™è¯¯ï¼š`å˜é‡ '<' æœªå®šä¹‰`

**é—®é¢˜ä¿®å¤åï¼š** ç¼–è¯‘æˆåŠŸ

```bash
python main.py batch examples/pl0_subset/programs test_outputs
```

è¾“å‡ºï¼š
```
[INFO] Total files:     3
[INFO] Success:         3  â† æ‰€æœ‰æ–‡ä»¶éƒ½æˆåŠŸï¼
[INFO] Errors:          0  â† æ²¡æœ‰é”™è¯¯ï¼
```

ç”Ÿæˆçš„ä¸­é—´ä»£ç ï¼ˆ`if_while_pl0.tac`ï¼‰ï¼š
```
x = 3
y = 5
max = x
max = y
param max
call write, 1
i = 0
param i
call write, 1
t1 = i + 1
i = t1
```

ç”Ÿæˆçš„ä¸­é—´ä»£ç ï¼ˆ`mixed_pl0.tac`ï¼‰ï¼š
```
n = 5
sum = 0
i = 1
t1 = sum + i
sum = t1
t2 = i + 1
i = t2
param sum
call write, 1
param n
call write, 1
```

### æµ‹è¯•5ï¼šé”™è¯¯æ£€æµ‹åŠŸèƒ½ âœ…

```bash
python main.py batch examples/error_test/pl0 test_outputs
```

è¾“å‡ºï¼š
```
[INFO] Total files:     20
[INFO] Success:         0
[INFO] Errors:          20  â† æ‰€æœ‰é”™è¯¯ç”¨ä¾‹éƒ½è¢«æ­£ç¡®æ£€æµ‹ï¼
```

**æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼** æ¶ˆé™¤ç¡¬ç¼–ç åï¼Œç³»ç»Ÿä¾ç„¶èƒ½æ­£ç¡®å¤„ç†ä¸åŒçš„è¯­è¨€ï¼ŒåŒ…æ‹¬é”™è¯¯æ£€æµ‹ï¼

---

## ä¼˜åŠ¿

1. **é›¶ç¡¬ç¼–ç **ï¼šæ‰€æœ‰tokenç±»å‹éƒ½ä»è¯æ³•è§„åˆ™æ–‡ä»¶ä¸­åŠ¨æ€æå–
2. **å®Œå…¨é€šç”¨**ï¼šæ”¯æŒä»»æ„è¯æ³•è§„åˆ™ï¼Œä¸é™äºç‰¹å®šè¯­è¨€
3. **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°tokenç±»å‹æ— éœ€ä¿®æ”¹ä»£ç 
4. **ç¬¦åˆç†è®º**ï¼šçœŸæ­£çš„ç¼–è¯‘å™¨ç¼–è¯‘å™¨è®¾è®¡

---

## ç¤ºä¾‹ï¼šæ”¯æŒä¸åŒçš„è¯æ³•å®šä¹‰

### ç¤ºä¾‹1ï¼šä½¿ç”¨ `ID` å’Œ `ASSIGN`
```
# lexer_rules.txt
ID = [a-zA-Z_][a-zA-Z0-9_]*
ASSIGN = =
```
âœ… è‡ªåŠ¨è¯†åˆ« `ID` ä¸ºæ ‡è¯†ç¬¦ï¼Œ`ASSIGN` ä¸ºæ“ä½œç¬¦

### ç¤ºä¾‹2ï¼šä½¿ç”¨ `VARIABLE` å’Œ `EQUALS`
```
# lexer_rules.txt
VARIABLE = [a-zA-Z_][a-zA-Z0-9_]*
EQUALS = =
```
âœ… è‡ªåŠ¨è¯†åˆ« `VARIABLE` ä¸ºæ ‡è¯†ç¬¦ï¼Œ`EQUALS` ä¸ºæ“ä½œç¬¦

### ç¤ºä¾‹3ï¼šä½¿ç”¨ä¸­æ–‡tokenåç§°
```
# lexer_rules.txt
æ ‡è¯†ç¬¦ = [a-zA-Z_][a-zA-Z0-9_]*
èµ‹å€¼ç¬¦ = =
```
âœ… è‡ªåŠ¨è¯†åˆ« `æ ‡è¯†ç¬¦` ä¸ºæ ‡è¯†ç¬¦ï¼Œ`èµ‹å€¼ç¬¦` ä¸ºæ“ä½œç¬¦

**å®Œå…¨ä¸éœ€è¦ä¿®æ”¹ä»£ç ï¼**

---

## æ–‡ä»¶ä¿®æ”¹æ¸…å•

1. âœ… `src/compiler_generator/parser_generator.py`
   - æ·»åŠ è¯æ³•è§„åˆ™å‚æ•°
   - æ·»åŠ tokenåˆ†ç±»æå–æ–¹æ³•
   - æ¶ˆé™¤æ‰€æœ‰ç¡¬ç¼–ç 
   - æ·»åŠ è¾…åŠ©æ–¹æ³•

2. âœ… `src/frontend/cli.py`
   - æ›´æ–° `create_parser_from_spec` è°ƒç”¨ï¼Œä¼ å…¥è¯æ³•è§„åˆ™

3. âœ… `docs/sdt_hardcode_analysis.md`
   - æ›´æ–°ç¡¬ç¼–ç åˆ†ææ–‡æ¡£

4. âœ… `docs/hardcode_elimination_summary.md`ï¼ˆæœ¬æ–‡æ¡£ï¼‰
   - åˆ›å»ºæ¶ˆé™¤æ€»ç»“æŠ¥å‘Š

---

## æ€»ç»“

æˆåŠŸå°†ç¼–è¯‘å™¨ç”Ÿæˆå™¨ä»"ç¡¬ç¼–ç ç‰¹å®šè¯­è¨€"å‡çº§ä¸º"çœŸæ­£çš„ç¼–è¯‘å™¨ç¼–è¯‘å™¨"ï¼

ç°åœ¨å¯ä»¥ï¼š
- æ”¯æŒä»»æ„è¯æ³•è§„åˆ™
- æ”¯æŒä»»æ„tokenç±»å‹åç§°
- æ”¯æŒä»»æ„è¯­è¨€å®šä¹‰
- å®Œå…¨ä¸éœ€è¦ä¿®æ”¹ä»£ç 

**è¿™æ‰æ˜¯çœŸæ­£çš„ç¼–è¯‘å™¨ç¼–è¯‘å™¨ï¼** ğŸ‰
