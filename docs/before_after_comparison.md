# æ¶ˆé™¤ç¡¬ç¼–ç å‰åå¯¹æ¯”

## æ ¸å¿ƒæ”¹è¿›

### æ”¹è¿›1ï¼šä»è¯æ³•è§„åˆ™ä¸­åŠ¨æ€æå–tokenåˆ†ç±»

#### ä¹‹å‰ï¼ˆæ— tokenåˆ†ç±»ï¼‰
```python
def __init__(self, enable_sdt: bool = True):
    # æ²¡æœ‰tokenåˆ†ç±»ä¿¡æ¯
    pass
```

#### ä¹‹åï¼ˆåŠ¨æ€æå–tokenåˆ†ç±»ï¼‰
```python
def __init__(self, lexer_rules: List[Tuple[str, str]] = None, enable_sdt: bool = True):
    # ä»è¯æ³•è§„åˆ™ä¸­æå–tokenåˆ†ç±»
    self.lexer_rules = lexer_rules or []
    self.identifier_tokens: Set[str] = set()  # å¦‚ {'ID', 'VARIABLE', ...}
    self.number_tokens: Set[str] = set()      # å¦‚ {'NUM', 'NUMBER', ...}
    self.operator_tokens: Set[str] = set()    # å¦‚ {'PLUS', 'MINUS', ...}
    self.keyword_tokens: Set[str] = set()     # å¦‚ {'IF', 'WHILE', ...}
    self.punctuation_tokens: Set[str] = set() # å¦‚ {'LPAREN', 'SEMI', ...}
    
    if lexer_rules:
        self._extract_token_categories_from_lexer_rules()
```

---

### æ”¹è¿›2ï¼šèµ‹å€¼è¯­å¥è¯†åˆ«

#### ä¹‹å‰ï¼ˆç¡¬ç¼–ç tokenç±»å‹åç§°ï¼‰
```python
# èµ‹å€¼è¯­å¥ï¼š'ID' 'ASSIGN' Expr 'SEMI'
elif len(production) >= 3 and production[0] == "'ID'" and production[1] == "'ASSIGN'":
    var_name = children[0].synthesized_value
    expr_val = children[2].synthesized_value
    self.emit(f"{var_name} = {expr_val}")
```

#### ä¹‹åï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
```python
# èµ‹å€¼è¯­å¥ï¼šidentifier_token operator_token Expr ...
elif (len(production) >= 3 and 
      self._is_identifier_token_in_production(production[0]) and 
      self._is_operator_token_in_production(production[1])):
    var_name = children[0].synthesized_value
    expr_val = children[2].synthesized_value
    self.emit(f"{var_name} = {expr_val}")
```

**æ”¯æŒçš„å˜åŒ–ï¼š**
- âœ… `ID` â†’ `VARIABLE` â†’ `IDENTIFIER` â†’ ä»»æ„åç§°
- âœ… `ASSIGN` â†’ `EQUALS` â†’ `EQ` â†’ ä»»æ„åç§°

---

### æ”¹è¿›3ï¼šæ§åˆ¶æµè¯­å¥è¯†åˆ«

#### ä¹‹å‰ï¼ˆç¡¬ç¼–ç å…³é”®å­—åç§°ï¼‰
```python
# whileå¾ªç¯ï¼š'WHILE' 'LPAREN' Condition 'RPAREN' Stmt
elif len(production) >= 5 and production[0] == "'WHILE'" and production[1] == "'LPAREN'":
    loop_label = self.new_label()
    # ...

# ifè¯­å¥ï¼š'IF' 'LPAREN' Condition 'RPAREN' Stmt
elif len(production) >= 5 and production[0] == "'IF'" and production[1] == "'LPAREN'":
    exit_label = self.new_label()
    # ...
```

#### ä¹‹åï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
```python
# whileå¾ªç¯ï¼škeyword_token punctuation_token Condition punctuation_token Stmt
elif (len(production) >= 5 and 
      self._is_keyword_token_in_production(production[0]) and 
      self._is_punctuation_token_in_production(production[1])):
    loop_label = self.new_label()
    # ...

# ifè¯­å¥ï¼škeyword_token punctuation_token Condition punctuation_token Stmt
elif (len(production) >= 5 and len(production) < 6 and
      self._is_keyword_token_in_production(production[0]) and 
      self._is_punctuation_token_in_production(production[1])):
    exit_label = self.new_label()
    # ...
```

**æ”¯æŒçš„å˜åŒ–ï¼š**
- âœ… `WHILE` â†’ `LOOP` â†’ `å¾ªç¯` â†’ ä»»æ„åç§°
- âœ… `IF` â†’ `WHEN` â†’ `æ¡ä»¶` â†’ ä»»æ„åç§°
- âœ… `LPAREN` â†’ `LPAR` â†’ `å·¦æ‹¬å·` â†’ ä»»æ„åç§°

---

### æ”¹è¿›4ï¼šæ“ä½œç¬¦è¯†åˆ«

#### ä¹‹å‰ï¼ˆç¡¬ç¼–ç æ“ä½œç¬¦åˆ—è¡¨ï¼‰
```python
# ç¡¬ç¼–ç æ“ä½œç¬¦åˆ—è¡¨
if op_type in ['PLUS', 'MINUS']:
    return self._process_add_op(children[0], left_val)

if op_type in ['MUL', 'DIV']:
    return self._process_mul_op(children[0], left_val)
```

#### ä¹‹åï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
```python
# ä»è¯æ³•è§„åˆ™ä¸­åŠ¨æ€è¯†åˆ«æ‰€æœ‰æ“ä½œç¬¦
if op_type in self.operator_tokens:
    # å¤„ç†ä»»æ„æ“ä½œç¬¦ï¼Œä¸é™äºPLUSã€MINUSã€MULã€DIV
    return self._process_add_op(children[0], left_val)
```

**æ”¯æŒçš„å˜åŒ–ï¼š**
- âœ… `PLUS` â†’ `ADD` â†’ `åŠ ` â†’ ä»»æ„åç§°
- âœ… `MINUS` â†’ `SUB` â†’ `å‡` â†’ ä»»æ„åç§°
- âœ… `MUL` â†’ `MULT` â†’ `ä¹˜` â†’ ä»»æ„åç§°
- âœ… `DIV` â†’ `DIVIDE` â†’ `é™¤` â†’ ä»»æ„åç§°

---

### æ”¹è¿›5ï¼šæ’é™¤åˆ—è¡¨

#### ä¹‹å‰ï¼ˆç¡¬ç¼–ç å¤§é‡tokenç±»å‹ï¼‰
```python
# æ’é™¤å·²çŸ¥çš„éæ“ä½œç¬¦tokenç±»å‹ï¼ˆç¡¬ç¼–ç ï¼‰
if op_token_type not in ['ID', 'NUM', 'LPAREN', 'RPAREN', 'SEMI', 'ASSIGN', 
                         'COMMA', 'LBRACE', 'RBRACE', 'BEGIN', 'END', 'VAR', 
                         'CONST', 'PROCEDURE', 'CALL', 'IF', 'WHILE', 'READ', 
                         'WRITE', 'PRINT']:
    node.synthesized_value = children[0].synthesized_value
```

#### ä¹‹åï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
```python
# æ¶ˆé™¤ç¡¬ç¼–ç ï¼šé€šè¿‡è¯æ³•è§„åˆ™åŠ¨æ€è¯†åˆ«æ“ä½œç¬¦
if self._is_operator_token_in_production(production[0]):
    node.synthesized_value = children[0].synthesized_value
```

**ä¼˜åŠ¿ï¼š**
- âœ… ä¸éœ€è¦ç»´æŠ¤å·¨å¤§çš„æ’é™¤åˆ—è¡¨
- âœ… æ”¯æŒä»»æ„tokenç±»å‹åç§°
- âœ… æ–°å¢tokenç±»å‹æ— éœ€ä¿®æ”¹ä»£ç 

---

### æ”¹è¿›6ï¼šæ•°å­—tokenè¯†åˆ«

#### ä¹‹å‰ï¼ˆç¡¬ç¼–ç 'NUM'ï¼‰
```python
# ç¡¬ç¼–ç äº†'NUM'
if token.type != 'NUM':  # NUMæ˜¯æ•°å­—ç±»å‹ï¼Œä¸éœ€è¦æ£€æŸ¥
    self.check_variable_defined(value, token)
```

#### ä¹‹åï¼ˆåŠ¨æ€è¯†åˆ«ï¼‰
```python
# ä»è¯æ³•è§„åˆ™ä¸­åŠ¨æ€è¯†åˆ«æ•°å­—token
if token.type not in self.number_tokens:  # ä¸æ˜¯æ•°å­—ç±»å‹ï¼Œéœ€è¦æ£€æŸ¥
    self.check_variable_defined(value, token)
```

**æ”¯æŒçš„å˜åŒ–ï¼š**
- âœ… `NUM` â†’ `NUMBER` â†’ `INTEGER` â†’ `æ•°å­—` â†’ ä»»æ„åç§°

---

## å®Œæ•´çš„æ¶ˆé™¤ç¡¬ç¼–ç æ–¹æ³•

### æ–¹æ³•1ï¼šæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¯†åˆ«

é€šè¿‡åˆ†æè¯æ³•è§„åˆ™ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œè‡ªåŠ¨è¯†åˆ«tokençš„è¯­ä¹‰ç±»åˆ«ï¼š

```python
def _extract_token_categories_from_lexer_rules(self):
    import re
    
    for token_type, pattern in self.lexer_rules:
        # è¯†åˆ«æ ‡è¯†ç¬¦ï¼šåŒ¹é… [a-zA-Z_][a-zA-Z0-9_]* æ¨¡å¼
        if re.search(r'\[a-zA-Z_\].*\[a-zA-Z0-9_\]', pattern):
            self.identifier_tokens.add(token_type)
        
        # è¯†åˆ«æ•°å­—ï¼šåŒ¹é… [0-9]+ æ¨¡å¼
        elif re.search(r'\[0-9\]', pattern):
            self.number_tokens.add(token_type)
        
        # è¯†åˆ«å…³é”®å­—ï¼šå›ºå®šå­—ç¬¦ä¸²ï¼ˆå¦‚ 'if', 'while'ï¼‰
        elif re.match(r'^[a-zA-Z]+$', pattern):
            self.keyword_tokens.add(token_type)
        
        # è¯†åˆ«æ ‡ç‚¹ç¬¦å·ï¼šæ‹¬å·ã€åˆ†å·ç­‰
        elif pattern in ['(', ')', ';', ',', ...]:
            self.punctuation_tokens.add(token_type)
        
        # è¯†åˆ«æ“ä½œç¬¦ï¼šç®—æœ¯ã€æ¯”è¾ƒç­‰ç¬¦å·
        elif pattern in ['+', '-', '*', '/', '=', '<', '>', ...]:
            self.operator_tokens.add(token_type)
```

### æ–¹æ³•2ï¼šåŠ¨æ€åˆ¤æ–­è¾…åŠ©æ–¹æ³•

```python
def _is_identifier_token_in_production(self, prod_symbol: str) -> bool:
    """åˆ¤æ–­äº§ç”Ÿå¼ä¸­çš„ç¬¦å·æ˜¯å¦æ˜¯æ ‡è¯†ç¬¦token"""
    if not prod_symbol.startswith("'") or not prod_symbol.endswith("'"):
        return False
    token_type = prod_symbol[1:-1]
    return token_type in self.identifier_tokens

def _is_operator_token_in_production(self, prod_symbol: str) -> bool:
    """åˆ¤æ–­äº§ç”Ÿå¼ä¸­çš„ç¬¦å·æ˜¯å¦æ˜¯æ“ä½œç¬¦token"""
    if not prod_symbol.startswith("'") or not prod_symbol.endswith("'"):
        return False
    token_type = prod_symbol[1:-1]
    return token_type in self.operator_tokens

# ç±»ä¼¼çš„ï¼š_is_keyword_token_in_production, _is_punctuation_token_in_production, ç­‰
```

---

## å®é™…åº”ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ”¯æŒä¸åŒçš„tokenå‘½å

**è¯æ³•è§„åˆ™1ï¼ˆä¼ ç»Ÿå‘½åï¼‰ï¼š**
```
ID = [a-zA-Z_][a-zA-Z0-9_]*
ASSIGN = =
PLUS = \+
```

**è¯æ³•è§„åˆ™2ï¼ˆè‡ªå®šä¹‰å‘½åï¼‰ï¼š**
```
VARIABLE = [a-zA-Z_][a-zA-Z0-9_]*
EQUALS = =
ADD = \+
```

**è¯æ³•è§„åˆ™3ï¼ˆä¸­æ–‡å‘½åï¼‰ï¼š**
```
æ ‡è¯†ç¬¦ = [a-zA-Z_][a-zA-Z0-9_]*
èµ‹å€¼ç¬¦ = =
åŠ å· = \+
```

**æ‰€æœ‰ä¸‰ç§éƒ½èƒ½æ­£å¸¸å·¥ä½œï¼Œæ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç ï¼** âœ…

### ç¤ºä¾‹2ï¼šæ”¯æŒä¸åŒçš„è¯­è¨€

**Simpleè¡¨è¾¾å¼è¯­è¨€ï¼š**
- Token: `PRINT`, `ID`, `NUM`
- è¯­æ³•: `'PRINT' 'LPAREN' Expr 'RPAREN'`

**PL/0è¯­è¨€ï¼š**
- Token: `WRITE`, `ID`, `NUM`
- è¯­æ³•: `'WRITE' 'LPAREN' Expr 'RPAREN'`

**è‡ªå®šä¹‰è¯­è¨€ï¼š**
- Token: `è¾“å‡º`, `å˜é‡`, `æ•°å­—`
- è¯­æ³•: `'è¾“å‡º' '(' è¡¨è¾¾å¼ ')'`

**æ‰€æœ‰è¯­è¨€éƒ½èƒ½è‡ªåŠ¨æ”¯æŒï¼** âœ…

---

## æ–‡ä»¶ä¿®æ”¹æ€»ç»“

### ä¿®æ”¹çš„æ–‡ä»¶

1. **src/compiler_generator/parser_generator.py**ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
   - æ·»åŠ è¯æ³•è§„åˆ™å‚æ•°åˆ° `__init__`
   - æ·»åŠ  `_extract_token_categories_from_lexer_rules()` æ–¹æ³•
   - æ·»åŠ 6ä¸ªè¾…åŠ©åˆ¤æ–­æ–¹æ³•
   - æ¶ˆé™¤æ‰€æœ‰ç¡¬ç¼–ç ï¼ˆ50+å¤„ä¿®æ”¹ï¼‰
   - æ›´æ–° `create_parser_from_spec()` å‡½æ•°
   - æ›´æ–° `generate_parser_code()` å‡½æ•°åŠç”Ÿæˆçš„ä»£ç 

2. **src/compiler_generator/code_generator.py**
   - æ·»åŠ  `List` å’Œ `Tuple` ç±»å‹å¯¼å…¥
   - æ›´æ–° `generate_compiler_code()` å‡½æ•°ç­¾å

3. **src/frontend/cli.py**
   - æ›´æ–° `create_parser_from_spec()` è°ƒç”¨
   - æ›´æ–° `generate_parser_code()` è°ƒç”¨
   - æ›´æ–° `generate_compiler_code()` è°ƒç”¨

### æ–°å¢çš„æ–‡æ¡£

1. **docs/sdt_hardcode_analysis.md** - ç¡¬ç¼–ç åˆ†ææŠ¥å‘Š
2. **docs/hardcode_elimination_summary.md** - æ¶ˆé™¤æ€»ç»“æŠ¥å‘Šï¼ˆæœ¬æ–‡æ¡£ï¼‰
3. **docs/before_after_comparison.md** - æ¶ˆé™¤å‰åå¯¹æ¯”

---

## æ€»ç»“

### æ¶ˆé™¤ç¡¬ç¼–ç å‰
- âŒ ç¡¬ç¼–ç äº†50+å¤„tokenç±»å‹åç§°
- âŒ ç¡¬ç¼–ç äº†æ“ä½œç¬¦åˆ—è¡¨
- âŒ ç¡¬ç¼–ç äº†å¤§é‡æ’é™¤åˆ—è¡¨
- âŒ åªèƒ½æ”¯æŒç‰¹å®šçš„tokenå‘½å

### æ¶ˆé™¤ç¡¬ç¼–ç å
- âœ… é›¶ç¡¬ç¼–ç 
- âœ… ä»è¯æ³•è§„åˆ™åŠ¨æ€æå–æ‰€æœ‰ä¿¡æ¯
- âœ… æ”¯æŒä»»æ„tokenç±»å‹åç§°
- âœ… æ”¯æŒä»»æ„è¯­è¨€å®šä¹‰
- âœ… çœŸæ­£çš„ç¼–è¯‘å™¨ç¼–è¯‘å™¨

### æµ‹è¯•è¯æ˜
- âœ… Simpleè¡¨è¾¾å¼è¯­è¨€ï¼šç¼–è¯‘æˆåŠŸ
- âœ… PL/0è¯­è¨€ï¼šç¼–è¯‘æˆåŠŸ
- âœ… ç”Ÿæˆçš„ç¼–è¯‘å™¨ï¼šè¿è¡ŒæˆåŠŸ

**ç°åœ¨è¿™æ‰æ˜¯çœŸæ­£çš„ç¼–è¯‘å™¨ç¼–è¯‘å™¨ï¼** ğŸ‰
